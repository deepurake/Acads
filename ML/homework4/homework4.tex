\documentclass[twoside,11pt]{article}\usepackage{amsmath,amsfonts,amsthm,fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\setlength{\parindent}{0pt}
\usepackage{graphicx}
\usepackage{bm}
\def\argmin{\operatornamewithlimits{arg\, min}}
\newcommand{\rbr}[1]{\left(#1\right)}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\Ncal}{\mathcal{N}}
\begin{document}

\title{CS 7641 CSE/ISYE 6740 Homework 4}
\author{Le Song}
\date{Deadline: 11/29 Sunday, 11:55 pm}
\maketitle

\begin{itemize}
  \item Submit your answers as an electronic copy on T-square.
  \item No unapproved extension of deadline is allowed. Late
  submission will lead to 0 credit.
  \item Typing with Latex is highly recommended. Typing with MS Word is also okay.
  If you handwrite, try to be clear as much as possible. No credit may be given to unreadable handwriting.
  \item Explicitly mention your collaborators if any. For the programming problem, it is
  absolutely not allowed to share your source code with anyone in the
  class as well as to use code from the Internet without reference.
  \item Recommended reading: PRML Section 13.2
 \end{itemize}
 

\section{Kernels [20 points]}
This problem will explore a number of kernels and non-kernels to get some more intuition for (i) what constitutes a valid kernel and (ii) see kind of functions we can implicitly define with kernels. Identify which of the followings is a valid kernel.  If it is a kernel, please write your answer explicitly as `True' and give mathematical proofs. If it is not a kernel, please write your answer explicitly as `False' and give explanations. 

 Suppose $K_1$ and $K_2$ are valid kernels (symmetric and positive definite) defined on $R^m\times R^m$.
\begin{enumerate}
\item $K(u,v) = \alpha K_1(u,v) + \beta K_2(u,v), \alpha,\beta\in R$.
\item $K(u,v) = u^\top C v$ where $C\in R^{m\times m}$.
\item $K(u,v) = K_1(f(u), f(v))$ where $f:R^m \rightarrow R^m$.
\item $K(u,v) = f(K_1(u,v))$ where $f$ is any polynomial with positive coefficients.
\item $K(u,v) = \exp{K_1(u,v)}$.
\item \begin{align}
	K(u,v) = \left\{
	\begin{aligned}
	&1 & \text{if } \|u-v\|_2 \leqslant 1\\
	& 0 & \text{otherwise}
	\end{aligned}
	\right.
	\end{align}
	
\item Suppose $K'$ is a valid kernel. 
	\begin{align}
	K(u,v) = \frac{K'(u,v)}{\sqrt{K'(u,u)K'(v,v)}}.
	\end{align}
	
\end{enumerate} 
 
 

\section{Markov Random Field, Conditional Random Field [20 pts]}

\textbf{[a-b]} A probability distribution on  3 discrete variables
a,b,c is defined by $P(a,b,c) = \frac{1}{Z}\psi(a,b,c) =
\frac{1}{Z}\phi_1(a,b)\phi_2(b,c)$, where the table for the two
factors are given below.

\begin{table}[!htb]

    \begin{minipage}{.5\linewidth}

      \centering

    \begin{tabular}{cc|c}
    a & b & $\phi_1(a,b)$ \\ \hline
    0 & 0 & 4             \\
    0 & 1 & 3             \\
    1 & 0 & 3             \\
    1 & 1 & 1             \\
    \end{tabular}

    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
     \begin{tabular}{cc|c}
    b & c & $\phi_2(b,c)$ \\ \hline
    0 & 0 & 3             \\
    0 & 1 & 2             \\
    0 & 2 & 1             \\
    1 & 0 & 4             \\
    1 & 1 & 1             \\
    1 & 2 & 3             \\
    \end{tabular}
    \end{minipage}
\end{table}

\subsubsection*{(a) Compute the slice of the joint factor $\psi(a,b,c)$ corresponding to $b = 1$. This is the table $\psi(a,b=1,c)$. [5 pts]}

\subsubsection*{(b) Compute $P(a = 1,b = 1)$. [5 pts]}


%\newpage

\iffalse
\textbf{[c-d]} Consider a probability distribution $P$ over four
binary variables $X_1$, $X_2$, $X_3$, and $X_4$ as below. The
distribution assigns probability $1/8$ to each of the following
configurations, and the probability 0 to all others. Let $H$ be an
undirected graph $X_1$--$X_2$--$X_3$--$X_4$--$X_1$.

\begin{table}[!h]
     \centering
    \begin{tabular}{cccc|c}
    \hline
    $X_1$ & $X_2$ & $X_3$   & $X_4$ & P   \\ \hline
    0     & 0     & 0       & 0     & 1/8 \\
    0     & 0     & 0       & 1     & 1/8 \\
    1     & 0     & 0       & 0     & 1/8 \\
    0     & 0     & 1       & 1     & 1/8 \\
    1     & 1     & 0       & 0     & 1/8 \\
    0     & 1     & 1       & 1     & 1/8 \\
    1     & 1     & 1       & 0     & 1/8 \\
    1     & 1     & 1       & 1     & 1/8 \\ \hline
    \multicolumn{4}{l|}{All other tuples}  & 0   \\ \hline
    \end{tabular}
\end{table}
\noindent

~\\
\subsubsection*{(c) Show that $P$ satisfies all of the global independencies implied by $H$. In other words, $I(H)$ is a subset of $I(P)$. [5 pts]}

\emph{Hint}: Test each independency numerically.

\subsubsection*{(d) Show that $P$ does not factorize over $H$. In other words, $P$ cannot be expressed as a product of potential functions defined over the cliques of $H$. [7 pts]}

\emph{Hint:} Use a proof by contradiction. Show that the joint
probability of a single assignment is not equal to the product of
the clique factors under that assignment.) Comment on the
consequences of allowing zero probabilities in Markov models.


~\\

\textbf{[e-f]} Let $H$ be a Markov network in variables $A, B, C, D,
E, F$, which is connected. Consider performing variable elimination,
with the elimination order $A, B, C, D, E, F$. Use the following
template to answer.

\begin{figure}[h!]
\centering
\includegraphics[width=30mm]{graph.png}
\label{overflow}
\end{figure}

\subsubsection*{(e) Draw in the missing edges for $H$ below, such that the number of fill edges added during variable
elimination is maximized. Draw the primary edges for $H$ as solid
lines, and the resulting fill edges as dashed lines. Draw the
resulting clique tree. Note that your solution for $H$ must result
in each node being connected to at least one other node.) [6 pts]}

\subsubsection*{(f) Repeat (e), so that the number of fill edges is minimized. Draw the resulting
clique tree. Note that each node must be connected to at least one
other node in $H$.) [6 pts]}

~\\
\fi

\subsubsection*{(c) Explain the difference between Conditional Random Fields and Hidden
Markov Models with respect to the following factors. Please give
only a one-line explanation. [10 pts]}

\begin{itemize}
  \item Type of model - generative/discriminative
  \item Objective function optimized
  \item Require a normalization constant
\end{itemize}
~\\~\\

\section{Hidden Markov Model [50 pts]}

This problem will let you get familiar with HMM algorithms by doing
the calculations by hand.

\textbf{[a-c]} There are three coins $(1,2,3)$, to throw them
randomly, and record the result. $S = {1,2,3}$; $V = {H,T}$ (Head or
Tail); $A, B, \pi$ is given as

\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
      \centering
A:
    \begin{tabular}{l|l|l|l}
    ~ & 1    & 2    & 3    \\\hline
    1 & 0.9  & 0.05 & 0.05 \\
    2 & 0.45 & 0.1  & 0.45 \\
    3 & 0.45 & 0.45 & 0.1  \\
    \end{tabular}

    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering

B:
    \begin{tabular}{l|l|l|l}
    ~ & 1   & 2    & 3    \\\hline
    H & 0.5 & 0.75 & 0.25 \\
    T & 0.5 & 0.25 & 0.75 \\
    \end{tabular}

    \end{minipage}

    \begin{minipage}{.5\linewidth}
      \centering
$\pi:$
    \begin{tabular}{llll}
    $\pi$ & 1/3 & 1/3 & 1/3 \\
    \end{tabular}
  \end{minipage}

\end{table}

\subsubsection*{(a) Given the model above, what's the probability of observation $O = {H,T,H}$. [10 pts]}

\iffalse
\subsubsection*{(b) Given the observation, what's the most likely sequence (the number of coin), how do you get the result? [5 pts]}
\fi
\subsubsection*{(b) Describe how to get the $A, B$, and $\pi$, when they are unknown. [10 pts]}


%\newpage
\iffalse
\textbf{[c-e]} Consider another HMM with three states,  three
outputs and the following transition and emission probabilities,
assume a uniform distribution for the initial state, $\pi_0$


\begin{table}[!htb]

    \begin{minipage}{.5\linewidth}

      \centering

    \begin{tabular}{ll|l|l|l}
    $\pi_i$ & $\pi_{i+1}$ & a   & b   & c   \\ \hline
    a       & ~           & 0.5 & 0.4 & 0.1 \\ \hline
    b       & ~           & 0.1 & 0.5 & 0.4 \\ \hline
    c       & ~           & 0.4 & 0.1 & 0.5 \\
    \end{tabular}

    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
   \begin{tabular}{ll|l|l|l}
    $\pi_i$ & $\pi_{i+1}$ & p   & q   & r   \\ \hline
    a       & ~           & 0.7 & 0.1 & 0.2 \\ \hline
    b       & ~           & 0.2 & 0.7 & 0.1 \\ \hline
    c       & ~           & 0.1 & 0.2 & 0.7 \\
    \end{tabular}
    \end{minipage}
\end{table}

\subsubsection*{(c) Compute the most likely sequence of hidden states for the observed sequence $(p, p, r, r, q,
r)$ by stepping through the Viterbi algorithm by hand. [4 pts]}

\subsubsection*{(d) Use the forward-backward algorithm to compute the probability distribution over states
at position 3. [4 pts]}

\subsubsection*{(e) Is the most likely state the same as the state in the most likely sequence? Will this
always be the case? Why? [4 pts]}

~\\
\fi

\subsubsection*{(c) In class, we studied discrete HMMs with discrete hidden states and
observations. The following problem considers a continuous density
HMM, which has discrete hidden states but continuous observations.
Let $S_t \in {1, 2, ..., n}$ denote the hidden state of the HMM at
time t, and let $X_t \in R$ denote the real-valued scalar
observation of the HMM at time t. In a continuous density HMM, the
emission probability must be parameterized since the random variable
$X_t$ is no longer discrete. It is defined as $P(X_t = x|S_t = i) =
\mathcal{N}(\mu_i,\sigma_i^2)$. Given $m$ sequences of observations
(each of length $T$), derive the EM algorithm for HMM with Gaussian
observation model. [14 pts]}


\subsubsection*{(d) For each of the following sentences, say whether it is true or false and provide a short explanation (one
sentence or so). [16 pts]}

\begin{itemize}
  \item The weights of all incoming edges to a state of an HMM must sum to 1.
  \item An edge from state $s$ to state $t$ in an HMM denotes the conditional probability of going to state s given that we are currently at state $t$.
  \item The "Markov" property of an HMM implies that we cannot use an HMM to model a process that depends on several time-steps in the past.
  \item The Baum-Welch algorithm is a type of an Expectation Maximization algorithm and as such it is guaranteed to converge to the (globally) optimal solution.
\end{itemize}


%\newpage
\section{Programming [30 pts]}

In this problem, you will implement  algorithm to analyze the
behavior of \emph{SP500} index over a period of time. For each week,
we measure the price movement relative to the previous week and
denote it using a binary variable (+1 indicates up and 1 indicates
down). The price movements from week 1 (the week of January 5) to
week 39 (the week of September 28) are plotted below.

Consider a Hidden Markov Model in which $x_t$ denotes the economic
state (good or bad) of week t and $y_t$ denotes the price movement
(up or down) of the \emph{SP500} index. We assume that
$x_{(t+1)}=x_t$ with probability 0.8, and
$P_{(Y_t|X_t)}(y_t=+1|x_t=\text{good}) =
P_{(Y_t|X_t)}(y_t=-1|x_t=\text{bad}) = q.$ In addition, assume that
$P_{(X_1)}(x_1=\text{bad}) = 0.8$. Load the \texttt{sp500.mat},
implement the algorithm, briefly describe how you implement this and
report the following :

\subsubsection*{(a) Assuming $q = 0.7$, plot $P_{(X_t|Y)}(x_t = \text{good}|y)$ for $t = 1,2,...,39$. What is the probability that the economy is in a good state in the week of week 39. [15 pts]}

\subsubsection*{(b) Repeat (a) for $q = 0.9$, and compare the result to that of (a). Explain your comparison in one or two sentences. [15 pts]}


\end{document}
